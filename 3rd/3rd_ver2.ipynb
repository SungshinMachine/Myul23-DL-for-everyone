{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599576724008",
   "display_name": "Python 3.7.6 64-bit (virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Linear Regression의 개념\n",
    "- *H(x) = Wx + b*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensorflow 2.x ver. on\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.__version__ <= \"1.15.0\":\n",
    "    print(\"tensorflow 1.x ver. on\")\n",
    "else:\n",
    "    print(\"tensorflow 2.x ver. on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "W = tf.Variable(tf.random.normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random.normal([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. model set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = lambda: tf.reduce_mean(tf.math.square(x_train*W + b - y_train))\n",
    "# optimizer = tf.compat.v1.train.GradientDescentPotimizer(learning_rate=0.01)\n",
    "optimizer = tf.keras.optimizers.SGD()   # default: learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "step:    0, cost:  0.671589, W:  1.483938, b: -0.249922\nstep:   20, cost:  0.025484, W:  1.192401, b: -0.356419\nstep:   40, cost:  0.017846, W:  1.157765, b: -0.350928\nstep:   60, cost:  0.016160, W:  1.147913, b: -0.335507\nstep:   80, cost:  0.014676, W:  1.140730, b: -0.319842\nstep:  100, cost:  0.013329, W:  1.134094, b: -0.304820\nstep:  120, cost:  0.012106, W:  1.127790, b: -0.290496\nstep:  140, cost:  0.010995, W:  1.121784, b: -0.276843\nstep:  160, cost:  0.009986, W:  1.116060, b: -0.263833\nstep:  180, cost:  0.009069, W:  1.110606, b: -0.251434\nstep:  200, cost:  0.008237, W:  1.105408, b: -0.239617\nstep:  220, cost:  0.007481, W:  1.100454, b: -0.228356\nstep:  240, cost:  0.006794, W:  1.095733, b: -0.217624\nstep:  260, cost:  0.006171, W:  1.091234, b: -0.207397\nstep:  280, cost:  0.005604, W:  1.086946, b: -0.197650\nstep:  300, cost:  0.005090, W:  1.082860, b: -0.188361\nstep:  320, cost:  0.004623, W:  1.078966, b: -0.179509\nstep:  340, cost:  0.004198, W:  1.075255, b: -0.171073\nstep:  360, cost:  0.003813, W:  1.071718, b: -0.163033\nstep:  380, cost:  0.003463, W:  1.068348, b: -0.155371\nstep:  400, cost:  0.003145, W:  1.065136, b: -0.148069\nstep:  420, cost:  0.002857, W:  1.062075, b: -0.141110\nstep:  440, cost:  0.002594, W:  1.059157, b: -0.134479\nstep:  460, cost:  0.002356, W:  1.056377, b: -0.128158\nstep:  480, cost:  0.002140, W:  1.053728, b: -0.122135\nstep:  500, cost:  0.001944, W:  1.051203, b: -0.116396\nstep:  520, cost:  0.001765, W:  1.048796, b: -0.110925\nstep:  540, cost:  0.001603, W:  1.046503, b: -0.105712\nstep:  560, cost:  0.001456, W:  1.044318, b: -0.100744\nstep:  580, cost:  0.001322, W:  1.042235, b: -0.096010\nstep:  600, cost:  0.001201, W:  1.040250, b: -0.091498\nstep:  620, cost:  0.001091, W:  1.038358, b: -0.087197\nstep:  640, cost:  0.000991, W:  1.036556, b: -0.083099\nstep:  660, cost:  0.000900, W:  1.034838, b: -0.079194\nstep:  680, cost:  0.000817, W:  1.033200, b: -0.075472\nstep:  700, cost:  0.000742, W:  1.031640, b: -0.071925\nstep:  720, cost:  0.000674, W:  1.030153, b: -0.068545\nstep:  740, cost:  0.000612, W:  1.028736, b: -0.065324\nstep:  760, cost:  0.000556, W:  1.027385, b: -0.062254\nstep:  780, cost:  0.000505, W:  1.026098, b: -0.059328\nstep:  800, cost:  0.000459, W:  1.024872, b: -0.056540\nstep:  820, cost:  0.000417, W:  1.023703, b: -0.053883\nstep:  840, cost:  0.000378, W:  1.022589, b: -0.051350\nstep:  860, cost:  0.000344, W:  1.021528, b: -0.048937\nstep:  880, cost:  0.000312, W:  1.020516, b: -0.046637\nstep:  900, cost:  0.000283, W:  1.019551, b: -0.044445\nstep:  920, cost:  0.000257, W:  1.018633, b: -0.042356\nstep:  940, cost:  0.000234, W:  1.017757, b: -0.040366\nstep:  960, cost:  0.000212, W:  1.016922, b: -0.038469\nstep:  980, cost:  0.000193, W:  1.016127, b: -0.036661\nstep: 1000, cost:  0.000175, W:  1.015369, b: -0.034938\nstep: 1020, cost:  0.000159, W:  1.014647, b: -0.033296\nstep: 1040, cost:  0.000144, W:  1.013958, b: -0.031731\nstep: 1060, cost:  0.000131, W:  1.013302, b: -0.030240\nstep: 1080, cost:  0.000119, W:  1.012677, b: -0.028819\nstep: 1100, cost:  0.000108, W:  1.012082, b: -0.027464\nstep: 1120, cost:  0.000098, W:  1.011514, b: -0.026174\nstep: 1140, cost:  0.000089, W:  1.010973, b: -0.024944\nstep: 1160, cost:  0.000081, W:  1.010457, b: -0.023771\nstep: 1180, cost:  0.000074, W:  1.009966, b: -0.022654\nstep: 1200, cost:  0.000067, W:  1.009497, b: -0.021590\nstep: 1220, cost:  0.000061, W:  1.009051, b: -0.020575\nstep: 1240, cost:  0.000055, W:  1.008626, b: -0.019608\nstep: 1260, cost:  0.000050, W:  1.008220, b: -0.018686\nstep: 1280, cost:  0.000045, W:  1.007834, b: -0.017808\nstep: 1300, cost:  0.000041, W:  1.007466, b: -0.016971\nstep: 1320, cost:  0.000038, W:  1.007115, b: -0.016174\nstep: 1340, cost:  0.000034, W:  1.006781, b: -0.015414\nstep: 1360, cost:  0.000031, W:  1.006462, b: -0.014689\nstep: 1380, cost:  0.000028, W:  1.006158, b: -0.013999\nstep: 1400, cost:  0.000026, W:  1.005869, b: -0.013341\nstep: 1420, cost:  0.000023, W:  1.005593, b: -0.012714\nstep: 1440, cost:  0.000021, W:  1.005330, b: -0.012117\nstep: 1460, cost:  0.000019, W:  1.005080, b: -0.011547\nstep: 1480, cost:  0.000017, W:  1.004841, b: -0.011005\nstep: 1500, cost:  0.000016, W:  1.004614, b: -0.010487\nstep: 1520, cost:  0.000014, W:  1.004397, b: -0.009995\nstep: 1540, cost:  0.000013, W:  1.004190, b: -0.009525\nstep: 1560, cost:  0.000012, W:  1.003993, b: -0.009077\nstep: 1580, cost:  0.000011, W:  1.003806, b: -0.008651\nstep: 1600, cost:  0.000010, W:  1.003627, b: -0.008244\nstep: 1620, cost:  0.000009, W:  1.003456, b: -0.007857\nstep: 1640, cost:  0.000008, W:  1.003294, b: -0.007487\nstep: 1660, cost:  0.000007, W:  1.003139, b: -0.007136\nstep: 1680, cost:  0.000007, W:  1.002992, b: -0.006800\nstep: 1700, cost:  0.000006, W:  1.002851, b: -0.006481\nstep: 1720, cost:  0.000005, W:  1.002717, b: -0.006176\nstep: 1740, cost:  0.000005, W:  1.002589, b: -0.005886\nstep: 1760, cost:  0.000005, W:  1.002468, b: -0.005609\nstep: 1780, cost:  0.000004, W:  1.002352, b: -0.005346\nstep: 1800, cost:  0.000004, W:  1.002241, b: -0.005095\nstep: 1820, cost:  0.000003, W:  1.002136, b: -0.004855\nstep: 1840, cost:  0.000003, W:  1.002036, b: -0.004627\nstep: 1860, cost:  0.000003, W:  1.001940, b: -0.004410\nstep: 1880, cost:  0.000003, W:  1.001849, b: -0.004203\nstep: 1900, cost:  0.000002, W:  1.001762, b: -0.004005\nstep: 1920, cost:  0.000002, W:  1.001679, b: -0.003817\nstep: 1940, cost:  0.000002, W:  1.001600, b: -0.003638\nstep: 1960, cost:  0.000002, W:  1.001525, b: -0.003467\nstep: 1980, cost:  0.000002, W:  1.001454, b: -0.003304\nstep: 2000, cost:  0.000001, W:  1.001385, b: -0.003149\n"
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    optimizer.minimize(cost, var_list=[W, b])\n",
    "    cost_ch = tf.reduce_mean(tf.math.square(x_train*W + b - y_train))\n",
    "    if step % 20 == 0:\n",
    "        print(\"step: %4d, cost: %9f, W: %9f, b: %9f\" %(step, cost_ch.numpy(), W.numpy(), b.numpy()), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번외) placeholder\n",
    "\n",
    "- tensorflow에선 함수의 매개변수화, 즉 cost 및 model을 함수화해서 사용하자.\n",
    "- 생각을 해보니 대체로 Machine Learning에선 train 데이터를 가지고 시작하기 때문에 굳이 구현해놓을 필요는 없을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cost(X, Y):\n",
    "    return tf.reduce_mean(tf.square(X*W + b - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 느낌인데 은근히 Eager스러운 코드 스타일이 있는지 관련 오류를 말해준다.<br />\n",
    "placeholder 이전에도 그런 오류가 떴는데 그때는 함수 내부에 식을 생으로 넣어서 해결했다. 이것 또한 그렇게 하기에는 placeholder의 취지를 너무 버리는 것 같았다."
   ]
  }
 ]
}